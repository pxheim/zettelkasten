[[$Research]]

Reply is token by token not word by word.

1. What are you.
2. Tell it what it should not do.
3. Then ask question.

RAG: Model has a training cutoff date. Does not know anything after that. Can provide information to the model using RAG. 

Tokens are not text, they are patterns in text, thus a string might consist of multiple tokens. Therefore `Presley` will be `Pres` and `ly`. Also, things like `The `, `the` `the ` will be different tokens.

"English is the new programming language" because of GPT and because it's optimized and most accurate for English.

Heteronyms make it hard for GPT. E.g. wind (as in air) and wind (as in winding something).

Temperature in GPT is a variable to change the distribution of the next token. With a temperature of 0, the replies are going to be very predictable. Higher temp are going to be more random. Most common is 0.55.

top p is another measure that can be changed. Similar to temperature, but it "removes" the tokens with low probabilities. With only temperature changed, you _can_ get very unlikely answers when increasing the temperature.

